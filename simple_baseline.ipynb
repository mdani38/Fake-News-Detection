{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from wordcloud import WordCloud\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize, FreqDist\n",
    "import PIL\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import gc\n",
    "sns.set()\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.read_csv('fakenewsnet_kaggle/BuzzFeed_fake_news_content.csv', delimiter=',')\n",
    "df1.dataframeName = 'BuzzFeed_fake_news_content.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.read_csv('fakenewsnet_kaggle/BuzzFeed_real_news_content.csv', delimiter=',')\n",
    "df2.dataframeName = 'BuzzFeed_real_news_content.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.text.iloc[88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('fakenewsnet_github/politifact_fake_updated.csv', delimiter=',')\n",
    "df4 = pd.read_csv('fakenewsnet_github/politifact_real_updated.csv', delimiter=',')\n",
    "df5 = pd.read_csv('fakenewsnet_github/gossipcop_fake_updated.csv', delimiter=',')\n",
    "df6 = pd.read_csv('fakenewsnet_github/gossipcop_real_updated.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 =df1.dropna(subset=['title', 'text'])\n",
    "df2 =df2.dropna(subset=['title', 'text'])\n",
    "df3 =df3.dropna(subset=['title', 'text'])\n",
    "df4 =df4.dropna(subset=['title', 'text'])\n",
    "df5 =df5.dropna(subset=['title', 'text'])\n",
    "df6 =df6.dropna(subset=['title', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index()\n",
    "df2 = df2.reset_index()\n",
    "df3 = df3.reset_index()\n",
    "df4 = df4.reset_index()\n",
    "df5 = df5.reset_index()\n",
    "df6 = df6.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflistreal = [df2,df4,df6]\n",
    "dflistfake = [df1,df3,df5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dflistreal:\n",
    "    df['target'] = 0\n",
    "    df['label'] = 'Real'\n",
    "for df in dflistfake:\n",
    "    df['target'] = 1\n",
    "    df['label'] = 'Fake'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.concat([df1, df2], sort = 'False')\n",
    "df_merge = pd.concat([df_merge, df3], sort = 'False')\n",
    "df_merge = pd.concat([df_merge, df4], sort = 'False')\n",
    "df_merge = pd.concat([df_merge, df5], sort = 'False')\n",
    "df_merge = pd.concat([df_merge, df6], sort = 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_merge.drop(columns=['id', 'index', 'Unnamed: 0', 'level_0','summary'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1, df2, df3, df4, df5, df6;\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['combined_text'] = df_merge.title + ' ' + df_merge.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "\n",
    "\n",
    "\n",
    "# Now we have a list that includes all english stopwords, as well as all punctuation\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``','”', \"`\",\"*\", \"’\", \"“\", \"‘\", \"–\", \"⋆\", \"'s\", \"—\", \"well\"]\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['title', 'text']\n",
    "target = df_merge['target']\n",
    "#cat_cols = ['source']\n",
    "#used_cols = text_cols \n",
    "#X = df_merge[used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_merge['combined_text'] ,target, test_size=0.20, random_state = 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_merge;\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import category_encoders as ce\n",
    "\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Preprocessing for text\n",
    "text_transformer = Pipeline(steps =[('vect', CountVectorizer(stop_words = stopwords_list)),\n",
    "('tfidf', TfidfTransformer())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "#categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#    ('loo', ce.leave_one_out.LeaveOneOutEncoder())\n",
    "#])   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('txt', text_transformer, text_cols)\n",
    "])\n",
    "model = MultinomialNB()\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                     ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "('vect', CountVectorizer(stop_words = stopwords_list)),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = text_clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "('vect', CountVectorizer(stop_words = stopwords_list)),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', RandomForestClassifier(n_estimators=100)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = text_clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "('vect', CountVectorizer(stop_words = stopwords_list)),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = text_clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, preds)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
